---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.7
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="8UTIGpTZZlOO" -->
# Imports
<!-- #endregion -->

```{python id="AnwSHO1L97I5"}
import requests
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime
from itertools import product
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import os
```

<!-- #region id="PVbrUy-T1IJZ" -->
# Configs
<!-- #endregion -->

<!-- #region id="NZ0XzZqV1IJZ" -->
## Constant Values
<!-- #endregion -->

```{python id="8BcPkym11IJZ"}
# Add your input and output path
INPUT_PATH = 'datasets/'

START_DATE = '2023-01-01'
END_DATE = '2023-04-30'

train_list = ['prev_day_demand', 'month', 'day_month', 'day_week']

test_size = 0.25
```

<!-- #region id="rGhdilSeZosS" -->
## Functions
<!-- #endregion -->

```{python id="4ZQ-EZpR1IJZ"}
def load_data(path, start_date: str, end_date: str):
    dataset = pd.read_parquet(path)
    start_date = datetime.date.fromisoformat(start_date)
    end_date = datetime.date.fromisoformat(end_date)
    clean_dataset = dataset[(dataset['tpep_pickup_datetime'].dt.date >= start_date) &
                            (dataset['tpep_pickup_datetime'].dt.date <= end_date)]
    # Add a new column for pick-up date
    clean_dataset['PU_date'] = pd.to_datetime(clean_dataset['tpep_pickup_datetime'].dt.date)
    return clean_dataset
```

```{python id="_WIm4Dc51IJa"}
def labeling(dataset):
    dataset_labels = dataset.groupby(['PULocationID', 'PU_date'])['PU_date'].count().to_frame('Demand')\
        .sort_values(['PULocationID', 'PU_date'], ascending=[True, True]).reset_index()\
        .rename(columns={'PULocationID': 'Location', 'PU_date': 'Date'})

    locations = pd.DataFrame(dataset_labels['Location'].unique(), columns=['Location'])
    dates = pd.DataFrame(dataset_labels['Date'].unique(), columns=['Date'])
    # create a DataFrame for all locations and dates
    loc_date_df = locations.merge(dates, how='cross').\
        sort_values(['Location', 'Date'], ascending=[True, True]).reset_index(drop=True)
    # Fill the DataFrame with demands
    labels_df = loc_date_df.merge(dataset_labels, how='left', on=['Location', 'Date']).fillna(value=0)

    return labels_df
```

```{python id="ERZPOS8I1IJa"}
def date_format_modifier(data):  # Year/Month/Day
    data['month'] = data['Date'].dt.month
    data['day_month'] = data['Date'].dt.day
    data['day_week'] = data['Date'].dt.dayofweek
    return data
```

```{python id="FUsylK9-1IJb"}
def train_test_splitting(dataset, test_size):
    start_date = dataset['Date'].min()
    end_date = dataset['Date'].max()
    all_days = end_date - start_date

    train_df = dataset[(dataset['Date'] - start_date) < (1 - test_size) * all_days]
    test_df = dataset[(dataset['Date'] - start_date) >= (1 - test_size) * all_days]

    return train_df, test_df
```

```{python id="BcGvcilUWEEC"}
def evaluate(metric, metric_name, true_values, predicted_values):
    print(f'{metric_name} : {metric(true_values, predicted_values)}')
```

```{python id="ifvk6uWS6hnT"}
def add_features(data):  # run after merging datasets and modifing date
    # Lag features
    data['prev_day_demand'] = data.groupby(['Location'])['Demand'].shift(1)

    data = data.dropna()

    return data
```

```{python id="sOMPo5ryBm8g"}
def grid_search(model, test_parameters, train_data, cv = None):
    gs = GridSearchCV(estimator = model, param_grid = test_parameters, scoring = 'neg_root_mean_squared_error', cv = cv, n_jobs = -1)
    gs.fit(train_data[train_list], train_data['Demand'])
    return gs.best_params_, gs.best_score_
```

```{python id="pbhM5Oe6PjW7"}
def model_predict(model, train_data, test_data):

    model.fit(train_data[train_list], train_data['Demand'])


    yhat_train = model.predict(train_data[train_list])
    yhat_test = model.predict(test_data[train_list])

    return yhat_train, yhat_test
```

```{python id="ao6nw8xsRvB9"}
def predict_visualizer(train_data, test_data, yhat_train, yhat_test):

    predicted_train_df = train_data.copy(deep = True)
    predicted_test_df = test_data.copy(deep = True)
    predicted_train_df['Predicted'] = yhat_train
    predicted_test_df['Predicted'] = yhat_test

    train_data = train_data.groupby('day_year')['Demand'].sum()
    test_data = test_data.groupby('day_year')['Demand'].sum()
    predicted_train_df = predicted_train_df.groupby('day_year')['Predicted'].sum()
    predicted_test_df = predicted_test_df.groupby('day_year')['Predicted'].sum()

    plt.title('Train')
    plt.plot(train_data)
    plt.plot(predicted_train_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()

    plot_length = len(test_data)
    plt.title('Test')
    plt.plot(test_data)
    plt.plot(predicted_test_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()
```

```{python id="v4-GWghuSbnA"}
def evaluation(model_name, train_data, test_data, yhat_train, yhat_test):
    print(f'{model_name} train scores:')


    evaluate(mean_absolute_error, 'MAE', train_data['Demand'], yhat_train)
    evaluate(mean_squared_error, 'MSE', train_data['Demand'], yhat_train)

    print(f'{model_name} test scores:')


    evaluate(mean_absolute_error, 'MAE', test_data['Demand'], yhat_test)
    evaluate(mean_squared_error, 'MSE', test_data['Demand'], yhat_test)

```

```{python id="dT43w3Uz1IJc"}
def day_of_year_modifier(dataset):
    dataset['day_year'] = 0
    start_date = dataset['Date'].min()

    for index, row in dataset.iterrows():
        # Get the day and month values from the current row
        curr_date = row['Date']
        day_number = (curr_date - start_date).days

        dataset.at[index, 'day_year'] = day_number
    return dataset
```

<!-- #region id="hj89WM701IJd" -->
# Load dataset
<!-- #endregion -->

```{python id="87BFHUu1-z73", outputId="7ce82d94-4855-489f-d0e7-c9bb685c4a23"}
rides_df = load_data(INPUT_PATH, START_DATE, END_DATE)
```

<!-- #region id="X2ES_CY6-fb5" -->
# Preprocess
<!-- #endregion -->

```{python id="VVRLakW_LeGp"}
labeled_df = labeling(rides_df)
```

```{python id="VJT59GlQ1IJe"}
featured_df = add_features(labeled_df)
```

```{python id="p9aUJr851IJe"}
dated_df = date_format_modifier(featured_df)
dated_df = day_of_year_modifier(dated_df)
```

```{python id="rBN2guud1IJe"}
train_df, test_df = train_test_splitting(dated_df, test_size)
```

```{python id="rOcOUqvL1IJe", outputId="2983accb-ce78-4e7d-d977-11808ca64d3f"}
train_df
```

<!-- #region id="7M2hxb3O1IJe" -->
# Model Training
<!-- #endregion -->

<!-- #region id="mIhvw9lH92sa" -->
## **Gradient Boosting Regressor**
<!-- #endregion -->

<!-- #region id="13gFIyYfZyxh" -->
### Tune Model
<!-- #endregion -->

```{python id="cm3YTQCMZVvu", outputId="4cf0ed3b-0243-476a-af87-35db760c9e5c"}
param_test = {'max_depth':range(2,10,2), 'min_child_weight':range(1,6,2)}

params = {"objective": "reg:squarederror", "tree_method": "gpu_hist", 'n_estimators':140, 'learning_rate':0.1, 'max_depth':5,
         'min_child_weight':1, 'gamma':0, 'subsample':0.8, 'colsample_bytree':0.8, 'seed':27}

best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

```{python id="f42LBpS81NB5", outputId="10c38a48-2453-478d-b9c3-ef14f69d0a97"}
params.update(best_params)
param_test = {'gamma':[i/10.0 for i in range(0,5)]}
best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

```{python id="Q3ueVbP51NB5", outputId="0559917d-9449-4877-e73a-19ee91f272ae"}
params.update(best_params)
param_test = {'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)]}
best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

```{python id="NKP1ofzt1NB5", outputId="019ff031-de3a-4b8d-f5ee-c1fd408300fd"}
params.update(best_params)
param_test = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}
best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

```{python id="mqXEz5h31NB6", outputId="a608106f-c416-4486-f047-1c0b29e00895"}
params.update(best_params)
param_test = {'n_estimators':list(range(1000, 5001, 500)), 'learning_rate':[0.01]}
best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

```{python id="fnIfCrYW1NB6", outputId="e3fdc3c1-3bd1-48ae-8ad5-c621ee1bc7c2"}
params.update(best_params)
param_test = {'n_estimators':list(range(15000, 20001, 500)), 'learning_rate':[0.001]}
best_params, best_score = grid_search(model = xgb.XGBRegressor(**params), test_parameters = param_test, train_data = train_df, cv = 5)
print(best_params, best_score)
```

<!-- #region id="H0IHB8t41NB6" -->
### Train With Tuned Parameters
<!-- #endregion -->

```{python id="5AdQtdSqN7m9"}
dtrain_reg = xgb.DMatrix(train_df[train_list].values, train_df['Demand'].values, enable_categorical=True)
dtest_reg = xgb.DMatrix(test_df[train_list].values, test_df['Demand'].values, enable_categorical=True)

#params.update(best_params)
params = {'learning_rate': 0.001, 'n_estimators': 15000, 'reg_alpha': 1, 'colsample_bytree': 0.6,
          'subsample': 0.9, 'gamma': 0.0, 'max_depth': 2, 'min_child_weight': 5}
model = xgb.XGBRegressor(**params)
yhat_train, yhat_test = model_predict(model, train_df, test_df)
```

<!-- #region id="J0EYl9KtTfo7" -->
### Visualization
<!-- #endregion -->

```{python id="deyLWLmZThMJ", outputId="4d0094aa-099c-490f-aa82-281ee16bae01"}
predict_visualizer(train_df, test_df, yhat_train, yhat_test)
```

<!-- #region id="lERphf0kTist" -->
### Evaluation
<!-- #endregion -->

```{python id="qSZwIFprTkqK", outputId="7ae94952-ec22-410a-9d26-6e05a96af6ab"}
evaluation('XGB', train_df, test_df, yhat_train, yhat_test)  # for whole data
```

```{python id="osueYsNP1NB8", outputId="b3c13ee6-bd74-4470-a8c5-558df5416e28"}
xgb.plot_importance(model)
plt.show()
```
