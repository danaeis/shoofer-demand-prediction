---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.7
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="8UTIGpTZZlOO" -->
# Imports
<!-- #endregion -->

```{python id="AnwSHO1L97I5"}
import catboost as cb
import datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shap

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import PCA
import geopandas as gpd
```

<!-- #region id="pSSMa3G2wGmF" -->
# Configs
<!-- #endregion -->

```{python id="zIkDLK9zwGmG"}
INPUT_PATH = 'datasets/'
ARIMA_PATH = 'statics/arima_predict.parquet'
OUTPUT_PATH = 'results/cb_predictions.parquet'

START_DATE = '2023-01-01'
END_DATE = '2023-05-15'

FEATURE_LIST = [#'ARIMA_predicts',
                
                #'Bronx',
                #'Brooklyn'	,
                #'EWR'	,
                #'Manhattan'	,
                #'Queens'	,
                #'Staten Island',
                
                #'Previous_week_trip_dist',
                #'Previous_2week_trip_dist',
                #'Previous_day_trip_dist',

                #'Previous_week_passenger_count',
                #'Previous_2week_passenger_count',
                #'Previous_day_passenger_count',

                #'Previous_week_fare_amount',
                #'Previous_day_fare_amount',
                #'Previous_2week_fare_amount',

                'Previous_2week_demand',
                'Previous_week_demand', 

                'Previous_day_demand',
                'Previous_2day_demand',
                'Previous_3day_demand',
                'Previous_4day_demand',
                'Previous_5day_demand',
                'Previous_6day_demand',

                #'Previous_week_extra',
                #'Previous_2week_extra',
                #'Previous_day_extra',
                
                #'Previous_2week_RatecodeID',
                #'Previous_week_RatecodeID',
                #'Previous_day_RatecodeID',

                #'Previous_week_congestion_surcharge',
                #'Previous_2week_congestion_surcharge',
                #'Previous_day_congestion_surcharge',

                #'Day_of_month', 
                'Day_of_week'
                ]

TEST_START_DATE = '2023-04-20'

AUTO_TUNE = False
```

<!-- #region id="5eS5BQE9wGmK" -->
# Data preparation

<!-- #endregion -->

<!-- #region id="TWsjXbpaznc_" -->
## Load Data
<!-- #endregion -->

```{python id="HAyNjqxNwGmH"}
def load_data(path, start_date: str, end_date: str):
    df = pd.read_parquet(path)
    start_date = datetime.date.fromisoformat(start_date)
    end_date = datetime.date.fromisoformat(end_date)
    filtered_df = df[(df['tpep_pickup_datetime'].dt.date >= start_date) &
                     (df['tpep_pickup_datetime'].dt.date <= end_date)]
    dataset = filtered_df.filter(items=['tpep_pickup_datetime', 'PULocationID', 'trip_distance', 'total_amount', 'passenger_count', 'fare_amount', 'extra', 
                                        'RatecodeID', 'congestion_surcharge', 'tip_amount'])
    dataset['PU_date'] = pd.to_datetime(dataset['tpep_pickup_datetime'].dt.date)
    return dataset
```

```{python id="87BFHUu1-z73"}
rides_df = load_data(INPUT_PATH, START_DATE, END_DATE)
```

```{python id="hCN-11QT3bp1"}
print(f'rides dataframe shape : {rides_df.shape}')
rides_df.head()
```

```{python}
arima_df = pd.read_parquet(ARIMA_PATH)
```

```{python}
print(f'statics dataframe shape : {arima_df.shape}')
arima_df.head()
```

```{python}
regions_df = gpd.read_file('taxi_zones/taxi_zones.shp')
regions_df = regions_df.rename(columns = {'LocationID' : 'Location'})
```

```{python}
regions_df['borough']
```

<!-- #region id="X2ES_CY6-fb5" -->
## Labeling
<!-- #endregion -->

```{python id="a7mNMQ-zwGmH"}
def labeling(dataset, zones):
    '''dataset_labels_demand = (
        dataset
        .groupby(['PULocationID', 'PU_date'])['PU_date']
        .count()
        .to_frame('Demand')
        .sort_values(['PULocationID', 'PU_date'], ascending=[True, True])
        .reset_index()
        .rename(columns={'PULocationID': 'Location', 'PU_date': 'Date'})
    )'''
    dataset_labels = (
                rides_df.groupby(['PULocationID', 'PU_date'])
                .agg({'trip_distance': 'sum', 'tpep_pickup_datetime': 'count', 'total_amount': 'max', 'passenger_count': 'median' ,'fare_amount': 'sum', 'extra': 'max',
                      'RatecodeID': 'max', 'congestion_surcharge': 'median', 'tip_amount': 'sum'})
                .reset_index()
                .rename(columns={'trip_distance': 'trip_dists', 'tpep_pickup_datetime': 'Demand', 'PULocationID': 'Location', 'PU_date': 'Date'})
)

    locations = pd.DataFrame(dataset_labels['Location'].unique(), columns=['Location'])
    dates = pd.DataFrame(dataset_labels['Date'].unique(), columns=['Date'])

    location_date_df = (
        locations
        .merge(dates, how='cross')
        .sort_values(['Location', 'Date'], ascending=[True, True])
        .reset_index(drop=True)
    )

    labels_df = (
        location_date_df
        .merge(dataset_labels, how='left', on=['Location', 'Date'])
        .merge(zones, how='left', on=['Location'])
        .fillna(value=0)
    )

    return labels_df

```

```{python id="VVRLakW_LeGp"}
labeled_df = labeling(rides_df, regions_df[['borough', 'Location']])
```

```{python}
labeled_df = pd.concat([labeled_df, pd.get_dummies(labeled_df["borough"])], axis=1)
labeled_df.drop("borough", axis=1, inplace=True)
labeled_df.drop(0, axis = 1, inplace = True)
```

```{python id="0Gm5j5em28Xy"}
print(f'labeled dataframe shape : {labeled_df.shape}')
labeled_df.head()
```

```{python}
labeled_df.to_parquet('labels.parquet')
```

<!-- #region id="G0W2pR-70JJb" -->
## Add Feature
<!-- #endregion -->

```{python id="ifvk6uWS6hnT"}
def feature_engineering(dataset, statics_df):
    dataset['Previous_day_demand'] = dataset.groupby(['Location'])['Demand'].shift(1)
    dataset['Previous_2day_demand'] = dataset.groupby(['Location'])['Demand'].shift(2)
    dataset['Previous_3day_demand'] = dataset.groupby(['Location'])['Demand'].shift(3)
    dataset['Previous_4day_demand'] = dataset.groupby(['Location'])['Demand'].shift(4)
    dataset['Previous_5day_demand'] = dataset.groupby(['Location'])['Demand'].shift(5)
    dataset['Previous_6day_demand'] = dataset.groupby(['Location'])['Demand'].shift(6)
    dataset['Previous_week_demand'] = dataset.groupby(['Location'])['Demand'].shift(7)
    dataset['Previous_2week_demand'] = dataset.groupby(['Location'])['Demand'].shift(14)

    dataset['Previous_week_passenger_count'] = dataset.groupby(['Location'])['passenger_count'].shift(7)
    dataset['Previous_2week_passenger_count'] = dataset.groupby(['Location'])['passenger_count'].shift(14)
    dataset['Previous_day_passenger_count'] = dataset.groupby(['Location'])['passenger_count'].shift(1)

    dataset['Previous_week_trip_dist'] = dataset.groupby(['Location'])['trip_dists'].shift(7)
    dataset['Previous_2week_trip_dist'] = dataset.groupby(['Location'])['trip_dists'].shift(14)
    dataset['Previous_day_trip_dist'] = dataset.groupby(['Location'])['trip_dists'].shift(1)

    dataset['Previous_week_tip_amount'] = dataset.groupby(['Location'])['tip_amount'].shift(7)
    dataset['Previous_2week_tip_amount'] = dataset.groupby(['Location'])['tip_amount'].shift(14)
    dataset['Previous_day_tip_amount'] = dataset.groupby(['Location'])['tip_amount'].shift(1)

    dataset['Previous_week_fare_amount'] = dataset.groupby(['Location'])['fare_amount'].shift(7)
    dataset['Previous_2week_fare_amount'] = dataset.groupby(['Location'])['fare_amount'].shift(14)
    dataset['Previous_day_fare_amount'] = dataset.groupby(['Location'])['fare_amount'].shift(1)

    dataset['Previous_week_extra'] = dataset.groupby(['Location'])['extra'].shift(7)
    dataset['Previous_2week_extra'] = dataset.groupby(['Location'])['extra'].shift(14)
    dataset['Previous_day_extra'] = dataset.groupby(['Location'])['extra'].shift(1)
    
    dataset['Previous_week_RatecodeID'] = dataset.groupby(['Location'])['RatecodeID'].shift(7)
    dataset['Previous_2week_RatecodeID'] = dataset.groupby(['Location'])['RatecodeID'].shift(14)
    dataset['Previous_day_RatecodeID'] = dataset.groupby(['Location'])['RatecodeID'].shift(1)

    dataset['Previous_week_congestion_surcharge'] = dataset.groupby(['Location'])['congestion_surcharge'].shift(7)
    dataset['Previous_2week_congestion_surcharge'] = dataset.groupby(['Location'])['congestion_surcharge'].shift(14)
    dataset['Previous_day_congestion_surcharge'] = dataset.groupby(['Location'])['congestion_surcharge'].shift(1)


    dataset['Day_of_week'] = dataset['Date'].dt.dayofweek   
    dataset['Day_of_month'] = dataset['Date'].dt.day

    dataset = dataset.sort_values(by = ['Date', 'Location'])
    dataset['ARIMA_predicts'] = None
    start_index = len(dataset) - len(statics_df)
    end_index = len(dataset)
    dataset.iloc[start_index:end_index, dataset.columns.get_loc('ARIMA_predicts')] = statics_df
    dataset['ARIMA_predicts'] = dataset['ARIMA_predicts'].astype('float')
    dataset = dataset.sort_values(by = ['Location', 'Date'])
    
    return dataset
```

```{python id="12zNfv8YwGmK"}
features_df = feature_engineering(labeled_df, arima_df)
features_df.dropna(inplace = True)
```

```{python id="bJxWEkiD3VOZ"}
print(f'features dataframe shape : {features_df.shape}')
features_df.head()
```

```{python}
pca = PCA()
pca.fit(features_df[FEATURE_LIST])


cumsum = np.cumsum(pca.explained_variance_ratio_)
cumsum, len(FEATURE_LIST), pca.feature_names_in_
```

<!-- #region id="zN0kp6jw03DP" -->
## Split Train and Test Data
<!-- #endregion -->

```{python id="CMY1G1lmwGmI"}
def train_test_splitting(dataset, TEST_START_DATE):

    train_df = dataset[dataset['Date'] < TEST_START_DATE]
    test_df = dataset[dataset['Date'] >= TEST_START_DATE]

    return train_df, test_df
```

```{python id="3xH4VMGNwGmK"}
train_df, test_df = train_test_splitting(features_df, TEST_START_DATE)
'''pca = PCA(n_components = 5)
xtrain = pca.fit_transform(train_df[FEATURE_LIST])

pca = PCA(n_components = 5)
xtest = pca.fit_transform(test_df[FEATURE_LIST])

pca.fit(features_df[FEATURE_LIST])'''
```

```{python id="xxGdRZfqwGmL"}
print(f'train dataframe shape : {train_df.shape}')
train_df.head()
```

```{python id="uPbINwH224Hy"}
print(f'test dataframe shape : {test_df.shape}')
test_df.head()
```

<!-- #region id="xf8ChW_7wGmL" -->
# Model Training
<!-- #endregion -->

<!-- #region id="mIhvw9lH92sa" -->
## **CatBoost Regressor**
<!-- #endregion -->

<!-- #region id="13gFIyYfZyxh" -->
### Model Tuning
<!-- #endregion -->

```{python id="sOMPo5ryBm8g"}
def grid_search(model, test_parameters, train_data, feature_list, cv = None):
    gs = GridSearchCV(
        estimator = model, 
        param_grid = test_parameters, 
        scoring = 'neg_root_mean_squared_error', 
        cv = cv, 
        n_jobs = -1
        )
    
    gs.fit(train_data[feature_list], train_data['Demand'])
    return gs.best_params_, gs.best_score_
```

```{python id="cm3YTQCMZVvu", outputId="4cf0ed3b-0243-476a-af87-35db760c9e5c"}
if AUTO_TUNE:
    params_test= {
    'depth': [4, 5],
    'learning_rate': [0.1, 0.7, 0.05, 0.03],
    'l2_leaf_reg': [6, 7, 8, 9, 10],
    'iterations': [150, 200, 250, 300],
    'early_stopping_rounds': [10, 20, 30]
}

    best_params, best_score = grid_search(
        model = cb.CatBoostRegressor(), 
        test_parameters = params_test,
        train_data = train_df, 
        feature_list = FEATURE_LIST, 
        cv = 3
        )
    
    print(best_params, best_score)
else:
    best_params = {'depth': 6, 
                   'early_stopping_rounds': 10, 
                   'iterations': 300, 
                   'l2_leaf_reg': 5, 
                   'learning_rate': 0.05}
    '''{'depth': 6, 
                   'early_stopping_rounds': 10, 
                   'iterations': 300, 
                   'l2_leaf_reg': 8, 
                   'learning_rate': 0.03}'''
```

<!-- #region id="H0IHB8t41NB6" -->
### Prediction
<!-- #endregion -->

```{python id="pbhM5Oe6PjW7"}
def model_predict(model, train_data, test_data, feature_list):

    model.fit(train_data[feature_list], train_data['Demand'])
    '''model.fit(train_data[feature_list], train_data['Demand'], 
            eval_set=[(test_data[feature_list], test_data['Demand'])], 
            early_stopping_rounds=100)
    print(np.min(model.evals_result()['validation_0']['rmse']))'''
    train_predict_df  = model.predict(train_data[feature_list])
    test_predict_df  = model.predict(test_data[feature_list])

    return train_predict_df, test_predict_df
```

```{python id="5AdQtdSqN7m9"}
model = cb.CatBoostRegressor(**best_params)
train_prediction_df, test_prediction_df = model_predict(model, train_df, test_df, FEATURE_LIST)
```

<!-- #region id="J0EYl9KtTfo7" -->
### Visualization
<!-- #endregion -->

```{python}
def add_day_of_year(data):
    data['day_year'] = 0
    start_date = data['Date'].min()

    for index, row in data.iterrows():

        curr_date = row['Date']
        day_number = (curr_date - start_date).days

        data.at[index, 'day_year'] = day_number
    return data
```

```{python id="ao6nw8xsRvB9"}
def prediction_visualization(train_data, test_data, train_prediction_df, test_prediction_df):

    train_data = add_day_of_year(train_data)
    test_data = add_day_of_year(test_data)

    predicted_train_df = train_data
    predicted_test_df = test_data
    predicted_train_df['Predicted'] = train_prediction_df
    predicted_test_df['Predicted'] = test_prediction_df

    train_data = train_data.groupby('day_year')['Demand'].sum()
    test_data = test_data.groupby('day_year')['Demand'].sum()
    predicted_train_df = predicted_train_df.groupby('day_year')['Predicted'].sum()
    predicted_test_df = predicted_test_df.groupby('day_year')['Predicted'].sum()

    plt.title('Train')
    plt.plot(train_data)
    plt.plot(predicted_train_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()

    plot_length = len(test_data)
    plt.title('Test')
    plt.plot(test_data)
    plt.plot(predicted_test_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()
```

```{python id="deyLWLmZThMJ", outputId="4d0094aa-099c-490f-aa82-281ee16bae01"}
prediction_visualization(train_df, test_df, train_prediction_df, test_prediction_df)
```

<!-- #region id="lERphf0kTist" -->
### Evaluation
<!-- #endregion -->

```{python id="BcGvcilUWEEC"}
def evaluate(metric, metric_name, true_values, predicted_values):
    print(f'{metric_name} : {metric(true_values, predicted_values)}')
```

```{python}
def smape(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_true) + np.abs(y_pred))
    smape = np.mean(numerator / denominator)
    return smape * 100
```

```{python id="v4-GWghuSbnA"}
def evaluation(model_name, train_df, test_df, train_prediction_df, test_prediction_df):
    print(f'{model_name} train scores:')


    evaluate(mean_absolute_error, 'MAE', train_df['Demand'], train_prediction_df)
    evaluate(mean_squared_error, 'MSE', train_df['Demand'], train_prediction_df)
    evaluate(mean_absolute_percentage_error, 'MAPE', train_df['Demand'], train_prediction_df)
    evaluate(smape, 'smape', train_df['Demand'], train_prediction_df)

    print(f'{model_name} test scores:')

    evaluate(mean_absolute_error, 'MAE', test_df['Demand'], test_prediction_df)
    evaluate(mean_squared_error, 'MSE', test_df['Demand'], test_prediction_df)
    evaluate(mean_absolute_percentage_error, 'MAPE', test_df['Demand'], test_prediction_df)
    evaluate(smape, 'smape', test_df['Demand'], test_prediction_df)

```

```{python id="qSZwIFprTkqK", outputId="7ae94952-ec22-410a-9d26-6e05a96af6ab"}
evaluation('XGB', train_df, test_df, train_prediction_df, test_prediction_df)
```

<!-- #region id="_286hlGi7VWD" -->
### Feature Importance
<!-- #endregion -->

```{python id="osueYsNP1NB8", outputId="b3c13ee6-bd74-4470-a8c5-558df5416e28"}
feature_importance = model.get_feature_importance()

# Create a list of feature names
feature_names = FEATURE_LIST

# Sort feature importance and feature names in descending order
sorted_indices = feature_importance.argsort()[::-1]
sorted_feature_importance = feature_importance[sorted_indices]
sorted_feature_names = [feature_names[i] for i in sorted_indices]

# Create a horizontal bar plot
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_feature_importance)), sorted_feature_importance, align='center')
plt.yticks(range(len(sorted_feature_importance)), sorted_feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('CatBoost Feature Importance')
plt.tight_layout()
plt.show()
```

# File Saving

```{python}
def save_predictions(dataset, path):
    dataset.to_parquet(path, index=False)
```

```{python}
def prediction_labeling(pred_df, labeled_df):
    pred_df = pd.DataFrame(pred_df, columns = ['Predicted_demand'])
    labeled_df.reset_index(inplace = True)
    labeled_prediction_df = labeled_df[['Location', 'Date']]
    labeled_prediction_df['Predicted_demand'] = pred_df
    return labeled_prediction_df
```

```{python}
labeled_prediction_df = prediction_labeling(test_prediction_df, test_df)
```

```{python}
print(f'labeled prediction dataframe shape : {labeled_prediction_df.shape}')
labeled_prediction_df.head()
```

```{python}
save_predictions(labeled_prediction_df, OUTPUT_PATH)
```
